---
title: "The Insurance Problem"
date: 2026-01-25
summary: "AI is uninsurable. Not because it's risky—but because no one can tell you who pays when it fails."
series: "Position One"
labels: ["Insurance", "Risk", "AI"]
tags: ["insurance", "liability", "risk", "ETLB"]
layout: "single"
---

Let me tell you what keeps insurance underwriters awake.

It's not catastrophic events. They can model those.  
It's not expensive claims. They can price those.  
It's not regulatory change. They can lobby those.

It's ambiguity.

Ambiguity is the one thing you cannot price.  
And AI is drenched in it.

---

Here's the problem, simply stated:

When an AI system causes harm, *who pays?*

- The developer who built it?
- The company that deployed it?
- The operator who approved its recommendation?
- The entity that trained it?
- The entity that provided the data?

Current answer: *Nobody knows.*

That's not a legal challenge. That's a pricing nightmare.

---

Insurance works by quantifying risk.

You take a population of events. You study their frequency. You estimate their cost. You add a margin. That's your premium.

This only works when the risk has boundaries.

Who is liable? → Determines who the policy covers.  
What is the maximum exposure? → Determines the coverage limit.  
When does liability attach? → Determines when the policy triggers.

For AI systems deployed in critical domains, every single one of these questions is currently... vibes.

Try writing that into an actuarial table.

---

The industry's current response is exclusion.

Look at any general liability policy. Look at any E&O policy. Look at any director's coverage.

Somewhere in the fine print, you'll find an AI exclusion. Or a "machine learning" exclusion. Or an "autonomous decision" exclusion.

This is not risk management. This is avoidance.

And avoidance doesn't scale. Because AI is already everywhere.

---

I talk to underwriters. 

They tell me the same thing: *"We want to cover AI risk. We just don't know how to define the insured."*

Think about that.

In every other domain—auto, property, professional liability—there's a clear party. The driver. The owner. The practitioner. One name. One policy. One exposure.

In AI-assisted decisions, there are five to fifteen parties, all with partial involvement, none with clear accountability.

How do you write a policy for that?

---

The answer is you don't.

You redesign the system.

---

Here's what would make AI insurable:

**One liable party, declared at execution time, with a defined scope.**

That's it. That's the whole structure.

- Who pays? → The liability bearer named at execution.  
- Maximum exposure? → The scope declared at execution.  
- When does liability attach? → At the moment of execution.

No ambiguity. No finger-pointing. No forensic narratives.

One name. One number. One timestamp.

Now you can write a policy.

---

I call this **Execution-Time Liability Binding.**

The rule:

> No system SHALL execute an irreversible action unless, at the moment of execution, exactly one natural person is bound as the liability bearer with an explicitly defined scope of exposure.

This is not a legal reform.  
This is a technical specification.  

Systems that comply are insurable.  
Systems that don't comply are... not.

Let the market sort it out.

---

Here's what this enables for insurers:

**1. Role-based pricing.**

You can create liability classes. Senior decision-makers bear higher scope, higher premiums. Junior operators bear lower scope, lower premiums. The risk matches the authority.

**2. Behavioral incentives.**

When a human knows their name is on the line, they pay attention. Claims frequency drops. Not because people get smarter—because the structure demands attention.

**3. Audit trails.**

Every execution has a cryptographic record. Claim investigation becomes deterministic. No more narratives. Just logs.

**4. Portfolio management.**

You can model AI liability like you model other professional liability. Same actuarial tools. Same statistical approaches. Just a new risk class.

---

The industry that moves first wins.

This is not a regulatory waiting game. Regulators are years behind.

The insurer that offers clear AI liability coverage—at competitive premiums—captures the enterprise market. Because enterprises *want* to deploy AI. They're just scared of the unquantified exposure.

Solve the ambiguity. Win the market.

---

I'm not selling you a product.

I'm offering a structural solution.

**Execution-Time Liability Binding** is a specification. It's language. It's architecture.

Any insurer can adopt it. Any enterprise can require it. Any regulator can mandate it.

The question is who acts first.

The underwriter who figures this out becomes the underwriter who defines the category.

Everyone else writes policies that reference their language.

---

Ambiguity is not a feature. It's a failure mode.

AI becomes insurable the moment liability becomes clear.

Liability becomes clear the moment execution binds to one human.

That's the whole chain.

You're welcome to wait.  
Or you can build the future.
