---
title: "Why Blame Cannot Be Diffused"
date: 2026-01-20
summary: "When AI makes a mistake, everyone points at everyone else. This is not confusion. This is design."
series: "Position One"
labels: ["Governance", "Enterprise", "Legal"]
tags: ["ETLB", "liability", "accountability"]
layout: "single"
---

Something went wrong.

The AI recommended. The human approved. The patient died. The money vanished. The bridge collapsed.

Now comes the meeting.

The vendor says: *"We provide tools, not decisions."*

The operator says: *"I followed the system's guidance."*

The manager says: *"I trusted my operator."*

The compliance team says: *"We validated the model."*

The board says: *"We set policy, not operations."*

Everyone points outward.

And somewhere, a family waits for someone to say: "It was me. I am responsible."

That person does not exist.

---

This is not a bug.

This is what happens when systems are designed without a single point of accountability. Every actor is individually rational. Every actor minimizes their exposure. Collectively, they produce a machine that harms without consequence.

I call this the **diffusion trap.**

The more people involved in a decision, the less anyone owns it. The more sophisticated the technology, the easier it is to hide behind complexity. The more layers of governance, the more fingerprints obscure the crime.

Committees are not accountability. Committees are *the avoidance* of accountability.

---

I have spent years watching this pattern.

I've seen it in hospitals where algorithms recommend and physicians rubber-stamp. I've seen it in banks where models score and humans click "approve." I've seen it in government where AI flags and bureaucrats defer.

The outcome is always the same.

When things go right, everyone claims credit.  
When things go wrong, everyone claims ignorance.

The system produces harm without producing a responsible party.

---

There is only one moment where accountability can be assigned.

Not before the decision—people hedge.  
Not after the damage—people lie.  

**At the moment of execution.**

That is the instant when the action is defined, the stakes are real, and the escape routes are closed.

Before that moment, everything is hypothetical.  
After that moment, everything is narrative.

Only at execution time can you answer truthfully: *Who chose this? Who accepted this risk? Who will pay if this breaks?*

---

The solution is not cultural.

You cannot train humans to be more honest under pressure. You cannot build ethical AI that absorbs blame. You cannot regulate your way to accountability when the structure prevents it.

What you can do is mandate a single field at execution time:

> **Who carries this consequence?**

One name. One human. One scope of exposure.

If that field is empty, execution stops.

---

This is not punitive.

The goal is not to find people to blame after disaster. The goal is to prevent the disasters that happen *because* no one is paying attention.

When someone's name is on the line, they pay attention.

When a committee's name is on the line, everyone relaxes.

That is the entire lesson of aviation, surgery, and nuclear operations. The industries where single-point accountability exists are the industries where catastrophic failure is rarest.

Not because the people are better.  
Because the structure demands attention.

---

I am not asking you to trust me.

I am asking you to notice what already works.

Every flight has one captain. Every surgery has one surgeon of record. Every nuclear action requires one human sign-off.

These industries did not invent single-point accountability through theory. They converged on it because failure taught them.

The question is whether we will learn the same lesson for AI—before or after the failures force us.

---

The objection I hear most often: *"But what if the human can't possibly understand the AI's decision?"*

Then the human should not approve it.

If a system is too complex for a human to carry liability, that system should not be making irreversible decisions. Not because complexity is bad—but because untraceable complexity is uninsurable.

Insurance is the market's way of pricing risk.  
If risk cannot be priced, it cannot be governed.  
If it cannot be governed, it should not be deployed.

---

Here is the rule I propose:

> **No system SHALL execute an irreversible action unless, at the moment of execution, exactly one natural person is bound as the liability bearer, with an explicitly defined scope.**

Read it again.

Not a committee. One person.  
Not before the action. At execution.  
Not unlimited exposure. Defined scope.

This is airtight. You cannot interpret it without breaking it.

---

The companies that adopt this first will win.

Not in reputation—though that helps.  
In insurance costs. In regulatory relationships. In litigation defense.

Clear accountability is cheaper than ambiguous accountability.  
The market will figure this out.

The question is whether you will be ahead of the market or behind it.

---

Accountability is not a value.

It is a structure.

You can believe in accountability all day long.  
If your structure diffuses it, your beliefs are decorative.

The only thing that matters is the answer to one question, at one moment:

**Who pays if this breaks?**

If no one can answer, no one will.  
And someone will pay anyway.  
Just not the right someone.
